# -*- coding: utf-8 -*-
"""new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kgFuaIgCy4WTa8ptNSyHyt7sPCBty4J6
"""

import torch
print("CUDA Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

!pip uninstall -y torch torchvision torchaudio xformers transformers gradio attrdict numpy mdtex2html tokenizers

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install xformers==0.0.21
!pip install git+https://github.com/huggingface/transformers
!pip install gradio attrdict numpy==1.26.4 mdtex2html tokenizers==0.15.2

!pip show xformers

!pip install xformers --no-cache-dir

!pip install -r https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL2/main/requirements.txt

!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2

import torch
print(torch.version.cuda)

!pip list

# Commented out IPython magic to ensure Python compatibility.
!rm -rf DeepSeek-VL2  # Delete any existing repo
!git clone https://github.com/deepseek-ai/DeepSeek-VL2
# %cd DeepSeek-VL2

!pip install -e .

import torch
import transformers
import xformers
import gradio
print("Torch Version:", torch.__version__)
print("Transformers Version:", transformers.__version__)
print("Xformers Version:", xformers.__version__)
print("Gradio Version:", gradio.__version__)

# Step 2: Verify GPU
import torch
print("CUDA Available:", torch.cuda.is_available())
print("Device:", torch.cuda.current_device() if torch.cuda.is_available() else "CPU")
print("Device Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A")

# Step 3: Modified app.py for Colab with debugging
import os
import gradio as gr
import torch
from PIL import Image
from deepseek_vl2.serve.inference import load_model, deepseek_generate, convert_conversation_to_prompts
from deepseek_vl2.models.conversation import SeparatorStyle
from deepseek_vl2.serve.app_modules.utils import configure_logger, strip_stop_words, pil_to_base64
from google.colab import files

logger = configure_logger()

MODELS = ["deepseek-ai/deepseek-vl2-tiny"]
DEPLOY_MODELS = {}
IMAGE_TOKEN = "<image>"

def fetch_model(model_name: str, dtype=torch.bfloat16):
    global DEPLOY_MODELS
    if model_name not in DEPLOY_MODELS:
        print(f"Loading {model_name}...")
        model_info = load_model(model_name, dtype=dtype)
        tokenizer, model, vl_chat_processor = model_info
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        DEPLOY_MODELS[model_name] = (tokenizer, model, vl_chat_processor)
        print(f"Loaded {model_name} on {device}")
    return DEPLOY_MODELS[model_name]

def generate_prompt_with_history(text, images, history, vl_chat_processor, tokenizer, max_length=2048):
    conversation = vl_chat_processor.new_chat_template()
    if history:
        conversation.messages = history
    if images:
        text = f"{IMAGE_TOKEN}\n{text}"
        text = (text, images)
    conversation.append_message(conversation.roles[0], text)
    conversation.append_message(conversation.roles[1], "")
    return conversation

def to_gradio_chatbot(conv):
    ret = []
    for i, (role, msg) in enumerate(conv.messages[conv.offset:]):
        if i % 2 == 0:
            if isinstance(msg, tuple):
                msg, images = msg
                for image in images:
                    img_b64 = pil_to_base64(image, "user upload", max_size=800, min_size=400)
                    msg = msg.replace(IMAGE_TOKEN, img_b64, 1)
            ret.append([msg, None])
        else:
            ret[-1][-1] = msg
    return ret

def predict(text, images, chatbot, history, model_name="deepseek-ai/deepseek-vl2-tiny"):
    print("Starting predict function...")
    tokenizer, vl_gpt, vl_chat_processor = fetch_model(model_name)
    if not text:
        print("Empty text input detected.")
        return chatbot, history, "Empty context."

    print("Processing images...")
    pil_images = [Image.open(img).convert("RGB") for img in images] if images else []
    conversation = generate_prompt_with_history(
        text, pil_images, history, vl_chat_processor, tokenizer
    )
    all_conv, _ = convert_conversation_to_prompts(conversation)
    stop_words = conversation.stop_str
    gradio_chatbot_output = to_gradio_chatbot(conversation)

    full_response = ""
    print("Generating response...")
    try:
        with torch.no_grad():
            for x in deepseek_generate(
                conversations=all_conv,
                vl_gpt=vl_gpt,
                vl_chat_processor=vl_chat_processor,
                tokenizer=tokenizer,
                stop_words=stop_words,
                max_length=2048,
                temperature=0.1,
                top_p=0.9,
                repetition_penalty=1.1
            ):
                full_response += x
                response = strip_stop_words(full_response, stop_words)
                conversation.update_last_message(response)
                gradio_chatbot_output[-1][1] = response
                print(f"Yielding partial response: {response[:50]}...")
                yield gradio_chatbot_output, conversation.messages, "Generating..."

        print("Generation complete.")
        torch.cuda.empty_cache()
        yield gradio_chatbot_output, conversation.messages, "Success"
    except Exception as e:
        print(f"Error in generation: {str(e)}")
        yield gradio_chatbot_output, conversation.messages, f"Error: {str(e)}"

# Gradio interface for OCR
def upload_and_process(image):
    if image is None:
        return "Please upload an image.", []
    prompt = "Extract all text from this image exactly as it appears, ensuring the output is in English only. Preserve spaces, bullets, numbers, and all formatting. Do not translate, generate, or include text in any other language."
    chatbot = []
    history = []
    print("Starting upload_and_process...")
    for chatbot_output, history_output, status in predict(prompt, [image], chatbot, history):
        print(f"Status: {status}")
        if status == "Success":
            return chatbot_output[-1][1], history_output
    return "Processing failed.", []

# Launch Gradio app
with gr.Blocks() as demo:
    gr.Markdown("### OCR Artwork to Text Extraction Application")
    image_input = gr.Image(type="filepath", label="Upload Image")
    output_text = gr.Textbox(label="Extracted Text")
    history_state = gr.State([])
    submit_btn = gr.Button("Extract Text")
    submit_btn.click(upload_and_process, inputs=image_input, outputs=[output_text, history_state])

demo.launch(share=True, debug=True)  # Added debug=True for more Gradio logs

# app_code =
# #UI changed as before application
# import torch
# import streamlit as st
# from PIL import Image
# from deepseek_vl2.serve.inference import load_model, deepseek_generate, convert_conversation_to_prompts
# from deepseek_vl2.serve.app_modules.utils import configure_logger, strip_stop_words, pil_to_base64

# # Set page config for wide layout
# st.set_page_config(layout="wide")

# # Set up logging
# logger = configure_logger()

# # Models and deployment
# MODELS = ["deepseek-ai/deepseek-vl2-tiny"]
# DEPLOY_MODELS = {}
# IMAGE_TOKEN = "<image>"

# # Fetch model
# def fetch_model(model_name: str, dtype=torch.bfloat16):
#     global DEPLOY_MODELS
#     if model_name not in DEPLOY_MODELS:
#         logger.info(f"Loading {model_name}...")
#         model_info = load_model(model_name, dtype=dtype)
#         tokenizer, model, vl_chat_processor = model_info
#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#         model = model.to(device).eval()  # Move to appropriate device
#         DEPLOY_MODELS[model_name] = (tokenizer, model, vl_chat_processor)
#         logger.info(f"Loaded {model_name} on {device}")
#     return DEPLOY_MODELS[model_name]

# # Generate prompt with history
# def generate_prompt_with_history(text, images, history, vl_chat_processor, tokenizer, max_length=2048):
#     conversation = vl_chat_processor.new_chat_template()
#     if history:
#         conversation.messages = history
#     if images:
#         text = f"{IMAGE_TOKEN}\n{text}"
#         text = (text, images)
#     conversation.append_message(conversation.roles[0], text)
#     conversation.append_message(conversation.roles[1], "")
#     return conversation

# # Convert conversation to gradio format (for compatibility with deepseek-vl2)
# def to_gradio_chatbot(conv):
#     ret = []
#     for i, (role, msg) in enumerate(conv.messages[conv.offset:]):
#         if i % 2 == 0:
#             if isinstance(msg, tuple):
#                 msg, images = msg
#                 for image in images:
#                     img_b64 = pil_to_base64(image, "user upload", max_size=800, min_size=400)
#                     msg = msg.replace(IMAGE_TOKEN, img_b64, 1)
#             ret.append([msg, None])
#         else:
#             ret[-1][-1] = msg
#     return ret

# # Predict function for OCR
# def predict(text, images, model_name="deepseek-ai/deepseek-vl2-tiny"):
#     logger.info("Starting predict function...")
#     tokenizer, vl_gpt, vl_chat_processor = fetch_model(model_name)
#     if not text:
#         logger.warning("Empty text input detected.")
#         return "Empty context."

#     logger.info("Processing images...")
#     pil_images = [Image.open(img).convert("RGB") for img in images] if images else []
#     conversation = generate_prompt_with_history(
#         text, pil_images, [], vl_chat_processor, tokenizer
#     )
#     all_conv, _ = convert_conversation_to_prompts(conversation)
#     stop_words = conversation.stop_str

#     full_response = ""
#     logger.info("Generating response...")
#     try:
#         with torch.no_grad():
#             for x in deepseek_generate(
#                 conversations=all_conv,
#                 vl_gpt=vl_gpt,
#                 vl_chat_processor=vl_chat_processor,
#                 tokenizer=tokenizer,
#                 stop_words=stop_words,
#                 max_length=2048,
#                 temperature=0.1,
#                 top_p=0.9,
#                 repetition_penalty=1.1
#             ):
#                 full_response += x
#         response = strip_stop_words(full_response, stop_words)
#         logger.info("Generation complete.")
#         return response
#     except Exception as e:
#         logger.error(f"Error in generation: {str(e)}")
#         return f"Error: {str(e)}"

# # OCR processing function
# def upload_and_process(image):
#     if image is None:
#         return "Please upload an image."
#     prompt = "Extract all text from this image exactly as it appears, ensuring the output is in English only. Preserve spaces, bullets, numbers, and all formatting. Do not translate, generate, or include text in any other language. Stop at the last character of the image text."
#     logger.info("Starting upload_and_process...")
#     extracted_text = predict(prompt, [image])
#     return extracted_text

# # Main UI
# st.markdown("<h1 style='text-align: center;'>🔍 Extract Job Info in One Click</h1>", unsafe_allow_html=True)

# uploaded_file = st.file_uploader("Upload an Image (PNG, JPG, JPEG)", type=["png", "jpg", "jpeg"])

# if uploaded_file:
#     extracted_text = upload_and_process(uploaded_file)
#     col1, col2 = st.columns(2)
#     with col1:
#         st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)
#     with col2:
#         st.markdown(
#             f"""
#             <div style="border: 1px solid #ccc; padding: 10px; width: 100%; white-space: pre-wrap; overflow: hidden; text-align: left;">
#                 {extracted_text}
#             </div>
#             """,
#             unsafe_allow_html=True
#         )
#     st.session_state["extracted_text"] = extracted_text

# errors_text = st.text_area("Paste Any Errors Here", height=200)
# rating = st.radio("Rate the OCR Extraction (5-1)", options=[5, 4, 3, 2, 1], index=None, horizontal=True)
# ref_number = st.text_input("Enter Reference Number", max_chars=10)

# if ref_number and not ref_number.isdigit():
#     st.warning("⚠ Reference Number must be a number.")
#     ref_number = ""

# if not ref_number or rating is None:
#     st.warning("⚠ Please enter a Reference Number and select a Rating to proceed.")
#     st.button("Submit", disabled=True)
# else:
#     if st.button("Submit"):
#         st.success("✅ Submitted successfully!")

# with open("app.py", "w") as f:
#     f.write(app_code)